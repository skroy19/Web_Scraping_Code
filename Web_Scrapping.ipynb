{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPqW6DNfDDPibzalJ28uEg9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Only for scrapping a page"],"metadata":{"id":"Ic0RWAJbypxR"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import csv\n","\n","# URL of the webpage to scrape\n","url = \"http://5000best.com/websites/\"\n","\n","# Fetch and parse the webpage\n","response = requests.get(url)\n","if response.status_code != 200:\n","    print(f\"Failed to fetch the webpage: {response.status_code}\")\n","else:\n","    soup = BeautifulSoup(response.content, 'html.parser')\n","\n","    # Locate the table with id \"ttable\"\n","    table = soup.find('table', {'id': 'ttable'})\n","    if not table:\n","        print(\"No table with id 'ttable' found.\")\n","    else:\n","        # Find all rows within the table body\n","        rows = table.find('tbody').find_all('tr')\n","        print(f\"Number of rows found: {len(rows)}\")\n","\n","        # Prepare the CSV file\n","        with open(\"websites_data.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n","            writer = csv.writer(file)\n","            writer.writerow([\"Category\", \"URL\", \"Description\"])  # Write the header row\n","\n","            # Iterate over each row to extract data\n","            for row in rows:\n","                columns = row.find_all('td')\n","                if len(columns) >= 8:  # Ensure there are enough columns\n","                    category = columns[2].text.strip()  # Column 3 for Category\n","                    url = columns[4].a['href'] if columns[4].find('a') else columns[4].text.strip()  # Column 5 for URL\n","                    description = columns[7].text.strip()  # Column 8 for Description\n","                    writer.writerow([category, url, description])\n","\n","        print(\"Data successfully scraped and saved to 'websites_data.csv'.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bvGGPeAmrGnY","executionInfo":{"status":"ok","timestamp":1734759632158,"user_tz":-360,"elapsed":2351,"user":{"displayName":"Sanjoy Kumar Roy","userId":"17385015598302473271"}},"outputId":"cd7abd33-2cbe-4299-dace-451b23c1525c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of rows found: 100\n","Data successfully scraped and saved to 'websites_data.csv'.\n"]}]},{"cell_type":"markdown","source":["For scrapping all page from the website"],"metadata":{"id":"jBUmI2pezRui"}},{"cell_type":"code","source":["#this actually make the dataset for website classification\n","import requests\n","from bs4 import BeautifulSoup\n","import csv\n","import time\n","\n","# Base URL of the website\n","base_url = \"http://5000best.com\"\n","\n","# Output CSV file\n","output_file = \"websites_data.csv\"\n","\n","# Prepare the CSV file\n","with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n","    writer = csv.writer(file)\n","    writer.writerow([\"Category\", \"URL\", \"Description\"])  # Write header row\n","\n","    # Start with the first page\n","    next_page_url = \"/websites/\"\n","    retries = 3  # Maximum retries for each page\n","\n","    while next_page_url:\n","        url = f\"{base_url}{next_page_url}\"\n","        print(f\"Scraping page: {url}\")\n","\n","        for attempt in range(retries):\n","            try:\n","                # Fetch the page\n","                # Need this timeout as the webpage reject if it gets so many request continuously\n","                response = requests.get(url, timeout=10)\n","                response.raise_for_status()  # Raise an error for HTTP codes >= 400\n","                break  # Exit retry loop if request is successful\n","            except requests.exceptions.RequestException as e:\n","                print(f\"Attempt {attempt + 1} failed: {e}\")\n","                if attempt < retries - 1:\n","                    time.sleep(5)  # Wait before retrying\n","                else:\n","                    print(f\"Failed to fetch {url} after {retries} attempts.\")\n","                    next_page_url = None  # Stop further scraping\n","                    break\n","        else:\n","            continue  # Move to the next iteration of the while loop\n","\n","        # Parse the page\n","        soup = BeautifulSoup(response.content, 'html.parser')\n","\n","        # Find the table with id \"ttable\"\n","        table = soup.find('table', {'id': 'ttable'})\n","        if not table:\n","            print(\"No table found on this page.\")\n","            break\n","\n","        # Extract rows from the table\n","        rows = table.find('tbody').find_all('tr')\n","        for row in rows:\n","            columns = row.find_all('td')\n","            if len(columns) >= 8:  # Ensure there are enough columns\n","                category = columns[2].text.strip()\n","                url = columns[4].a['href'] if columns[4].find('a') else columns[4].text.strip()\n","                description = columns[7].text.strip()\n","                writer.writerow([category, url, description])\n","\n","        # Find the \"Next\" page link\n","        pagination = soup.find('div', {'id': 'dpages'})\n","        if pagination:\n","            current_page_num = int(next_page_url.split('/')[-2]) if next_page_url.split('/')[-2].isdigit() else 1\n","            next_link = pagination.find('a', text=f\"[{current_page_num + 1}]\")\n","            if next_link:\n","                next_page_url = next_link['href']\n","            else:\n","                next_page_url = None\n","        else:\n","            next_page_url = None\n","\n","        # Delay between requests\n","        time.sleep(5)\n","\n","print(f\"Data successfully scraped and saved to '{output_file}'.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KNyZnmKtvaC4","executionInfo":{"status":"ok","timestamp":1734760275199,"user_tz":-360,"elapsed":329746,"user":{"displayName":"Sanjoy Kumar Roy","userId":"17385015598302473271"}},"outputId":"411bd89f-d8be-49eb-9fd9-83ca98bce350"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Scraping page: http://5000best.com/websites/\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-23-f5a335e19207>:65: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n","  next_link = pagination.find('a', text=f\"[{current_page_num + 1}]\")\n"]},{"output_type":"stream","name":"stdout","text":["Scraping page: http://5000best.com/websites/2/\n","Scraping page: http://5000best.com/websites/3/\n","Scraping page: http://5000best.com/websites/4/\n","Scraping page: http://5000best.com/websites/5/\n","Scraping page: http://5000best.com/websites/6/\n","Scraping page: http://5000best.com/websites/7/\n","Attempt 1 failed: HTTPConnectionPool(host='5000best.com', port=80): Max retries exceeded with url: /websites/7/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7deb1a33ef50>: Failed to establish a new connection: [Errno 111] Connection refused'))\n","Scraping page: http://5000best.com/websites/8/\n","Scraping page: http://5000best.com/websites/9/\n","Scraping page: http://5000best.com/websites/10/\n","Scraping page: http://5000best.com/websites/11/\n","Scraping page: http://5000best.com/websites/12/\n","Scraping page: http://5000best.com/websites/13/\n","Scraping page: http://5000best.com/websites/14/\n","Scraping page: http://5000best.com/websites/15/\n","Scraping page: http://5000best.com/websites/16/\n","Attempt 1 failed: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n","Attempt 2 failed: HTTPConnectionPool(host='5000best.com', port=80): Max retries exceeded with url: /websites/16/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7deb1c297040>: Failed to establish a new connection: [Errno 111] Connection refused'))\n","Scraping page: http://5000best.com/websites/17/\n","Scraping page: http://5000best.com/websites/18/\n","Scraping page: http://5000best.com/websites/19/\n","Scraping page: http://5000best.com/websites/20/\n","Scraping page: http://5000best.com/websites/21/\n","Scraping page: http://5000best.com/websites/22/\n","Scraping page: http://5000best.com/websites/23/\n","Scraping page: http://5000best.com/websites/24/\n","Scraping page: http://5000best.com/websites/25/\n","Scraping page: http://5000best.com/websites/26/\n","Attempt 1 failed: HTTPConnectionPool(host='5000best.com', port=80): Max retries exceeded with url: /websites/26/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7deb1a4670d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n","Scraping page: http://5000best.com/websites/27/\n","Scraping page: http://5000best.com/websites/28/\n","Scraping page: http://5000best.com/websites/29/\n","Scraping page: http://5000best.com/websites/30/\n","Scraping page: http://5000best.com/websites/31/\n","Scraping page: http://5000best.com/websites/32/\n","Scraping page: http://5000best.com/websites/33/\n","Scraping page: http://5000best.com/websites/34/\n","Scraping page: http://5000best.com/websites/35/\n","Scraping page: http://5000best.com/websites/36/\n","Attempt 1 failed: HTTPConnectionPool(host='5000best.com', port=80): Max retries exceeded with url: /websites/36/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7deb1cba83d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n","Scraping page: http://5000best.com/websites/37/\n","Scraping page: http://5000best.com/websites/38/\n","Scraping page: http://5000best.com/websites/39/\n","Scraping page: http://5000best.com/websites/40/\n","Scraping page: http://5000best.com/websites/41/\n","Scraping page: http://5000best.com/websites/42/\n","Scraping page: http://5000best.com/websites/43/\n","Scraping page: http://5000best.com/websites/44/\n","Scraping page: http://5000best.com/websites/45/\n","Attempt 1 failed: HTTPConnectionPool(host='5000best.com', port=80): Max retries exceeded with url: /websites/45/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7deb1c9666b0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n","Attempt 2 failed: HTTPConnectionPool(host='5000best.com', port=80): Max retries exceeded with url: /websites/45/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7deb1c966fe0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n","Scraping page: http://5000best.com/websites/46/\n","Scraping page: http://5000best.com/websites/47/\n","Scraping page: http://5000best.com/websites/48/\n","Scraping page: http://5000best.com/websites/49/\n","Scraping page: http://5000best.com/websites/50/\n","Data successfully scraped and saved to 'websites_data.csv'.\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Replace 'websites_data.csv' with the correct path to your CSV file\n","file_path = \"websites_data.csv\"\n","\n","# Read the CSV file\n","csv_data = pd.read_csv(file_path)\n","\n","# Display the content of the CSV file\n","print(csv_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XE_pOyuela7i","executionInfo":{"status":"ok","timestamp":1734760293888,"user_tz":-360,"elapsed":385,"user":{"displayName":"Sanjoy Kumar Roy","userId":"17385015598302473271"}},"outputId":"45887217-509d-40b8-c943-c29835b57fbb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["        Category                            URL  \\\n","0      Discovery              http://google.com   \n","1         People            http://facebook.com   \n","2         Videos             http://youtube.com   \n","3        Portals               http://yahoo.com   \n","4     Discussion             http://twitter.com   \n","...          ...                            ...   \n","4995         Web         http://directtrack.com   \n","4996        Porn               http://cliti.com   \n","4997         Web           http://ispionage.com   \n","4998      Videos            http://videobam.com   \n","4999   Discovery  http://5000best.com/websites/   \n","\n","                                            Description  \n","0     Google - Search the world's information, inclu...  \n","1     Welcome to Facebook - Facebook helps you conne...  \n","2     YouTube - Share your videos with friends, fami...  \n","3                                                Yahoo!  \n","4     Twitter - Instantly connect to what's most imp...  \n","...                                                 ...  \n","4995  DirectTrack Affiliate Marketing & Tracking Pla...  \n","4996  Cliti - Free Porn Tubes - Full Free Porn Tube ...  \n","4997  Keyword Tool | Keyword Research and Tracking T...  \n","4998  Fast, Free Video Hosting & Video Sharing - Vid...  \n","4999                                 5000 Best Websites  \n","\n","[5000 rows x 3 columns]\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the scraped CSV file\n","file_path = \"websites_data.csv\"\n","\n","# Read the CSV file\n","try:\n","    data = pd.read_csv(file_path)\n","\n","    # Count the number of websites per category\n","    category_counts = data['Category'].value_counts()\n","\n","    # Display the category counts\n","    print(\"Category-wise Website Counts:\")\n","    print(category_counts)\n","\n","    # Save the results to a CSV file\n","    category_counts.to_csv(\"category_counts.csv\", header=[\"Count\"])\n","    print(\"Category counts have been saved to 'category_counts.csv'.\")\n","except FileNotFoundError:\n","    print(\"The file 'websites_data.csv' was not found. Please ensure the file exists and try again.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ed99bbNfz2sJ","executionInfo":{"status":"ok","timestamp":1734760954285,"user_tz":-360,"elapsed":379,"user":{"displayName":"Sanjoy Kumar Roy","userId":"17385015598302473271"}},"outputId":"bbe8c1b6-13c7-46a2-8322-d6cace132bd6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Category-wise Website Counts:\n","Category\n","Web              786\n","Commerce         405\n","News             312\n","Games            247\n","Technology       225\n","Education        221\n","Tools            207\n","Porn             201\n","Services         199\n","Style            172\n","Files            168\n","Pictures         161\n","Business         155\n","Articles         146\n","Health           138\n","Travel           135\n","Entertainment    117\n","Sport            114\n","Music            106\n","Discussion        96\n","Videos            82\n","Organizations     76\n","TV                70\n","People            68\n","Discovery         65\n","Cars              61\n","Humor             60\n","Jobs              59\n","Books             48\n","Movies            47\n","Science           31\n","Portals           22\n","Name: count, dtype: int64\n","Category counts have been saved to 'category_counts.csv'.\n"]}]}]}